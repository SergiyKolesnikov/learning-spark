{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a97f35-32ca-4e73-b0a0-5f8763b8c211",
   "metadata": {},
   "source": [
    "# Chapter 8. Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcf7a7-ba94-4f12-a75e-4db2a936afc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from uuid import uuid1\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  # Add Kafka-source library.  The version after \":\" must be the Kafka version that you usew\n",
    "  .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\")\n",
    "  .master(\"local[4]\")\n",
    "  .appName(\"StructuredStreaming\")\n",
    "  .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af97418-4748-4a08-852f-a9f88eb4db6d",
   "metadata": {},
   "source": [
    "## The Fundamentals of a Structured Streaming Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f951e-ef74-498e-b2e1-f67bdf4d7665",
   "metadata": {},
   "source": [
    "For the following streaming query to work, we need a TCP server that will listen at `127.0.0.1:61080` and will be sending text lines.\n",
    "\n",
    "We can use `netcat-openbsd` for this. In a terminal run `nc -lk -s 127.0.0.1 -p 61080` and start typing text lines. Observe the output in this notebook. It should be something like this\n",
    "\n",
    "```\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+----+-----+\n",
    "|word|count|\n",
    "+----+-----+\n",
    "| foo|    1|\n",
    "+----+-----+\n",
    "```\n",
    "\n",
    "To terminate the query interrupt the Jupyter kernel (menu Krenel -> Interrupt Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adc70b-a77c-4ada-80ad-ad0c5d8832f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random checkpoint dirname. Ust it if you want every query to start anew.\n",
    "checkpoint_dir = f\"/tmp/spark-streaming-checkpoints-{uuid1()}\"\n",
    "\n",
    "# Static checkpoint dirname. Use it if you want to restart a stopped query.\n",
    "# checkpoint_dir = f\"./spark-streaming-checkpoints\"\n",
    "\n",
    "# Step 1: Define input sources \n",
    "lines = (spark\n",
    "         .readStream\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"127.0.0.1\")\n",
    "         .option(\"port\", \"61080\")\n",
    "         .load())\n",
    "# Step 2: Transform data\n",
    "words = lines.select(F.explode(F.split(F.col(\"value\"), \"\\\\s\")).alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "# Step 3: Define output sink and output mode\n",
    "writer = (counts\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\"))\n",
    "# Step 4: Specify processing details\n",
    "writer2 = (writer\n",
    "           .trigger(processingTime=\"1 second\")\n",
    "           .option(\"checkpointLocation\", checkpoint_dir))\n",
    "# Step 5: Start the query\n",
    "streaming_query = writer2.start()\n",
    "# The following line will block for 60 seconds and the console output will be echoed in this notebook\n",
    "# in the cell output. You can unblock earlier by interrupting the Jupyter kernel (menu Krenel -> Interrupt Kernel)\n",
    "streaming_query.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566eba32-aba3-4fbd-8e17-81e1670125c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The streaming query is still running. You can still observe the console output\n",
    "# in the terminal in which you started Jupyter.\n",
    "streaming_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7366db9-43f2-44ed-aedd-8d26d516db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()\n",
    "streaming_query.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6be4d-1b6a-4591-a13d-6eb7d8fcc4ff",
   "metadata": {},
   "source": [
    "Now the query is stopped.\n",
    "\n",
    "If you used a static checkpoint dirname, you can restart the query from the point where it left off. To restart the query, reexecute the cell that creates and starts the streaming query (with steps 1 to 5). You may get \"ERROR MicroBatchExecution\" with IndexOutOfBoundsException. In this case rerun the cell one more time.\n",
    "\n",
    "**NOTE:** If you use a static checkpoint dirname and you stopped and restart netcat inbetween, your restarted query may stop accepting input from netcat. In this case you may need a complete reset: stop the query, remove the checkpoint directory manually, restart netcat, and finaly restart the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456856d-6ed7-4372-844b-392a61136063",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28eaf8-613c-4570-9eae-29a39fca3da2",
   "metadata": {},
   "source": [
    "## Streaming Data Sources and Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001d24f-5e8a-41aa-a6ee-35977551eacf",
   "metadata": {},
   "source": [
    "### Reading from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09255a1c-4863-44aa-b009-bf3c085a1d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory_of_json_files = \"../data/streaming_json\"\n",
    "file_schema_read_json = \"`key` integer, `value` string\"\n",
    "\n",
    "df_read_json = (spark\n",
    "           .readStream\n",
    "           .format(\"json\")\n",
    "           .schema(file_schema_read_json)\n",
    "           .load(input_directory_of_json_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c9a0e-1cd1-4cd9-8727-c9c021fde273",
   "metadata": {},
   "source": [
    "After starting the query in the next cell you will see the data from the file `00.json` in the cell output. Create a new file by copying `00.json` to `1.json`:\n",
    "```\n",
    "cp data/streaming_json/00.json data/streaming_json/1.json\n",
    "```\n",
    "and you will see the same data output again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19c697-0acf-4d15-8d70-6ebceeb6a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_read_json = f\"./spark-streaming-checkpoints-read-json\"\n",
    "\n",
    "streaming_query_read_json = (df_read_json\n",
    "                        .writeStream\n",
    "                        .format(\"console\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .trigger(processingTime=\"1 second\")\n",
    "                        .option(\"checkpointLocation\", checkpoint_dir_read_json)\n",
    "                        .start())\n",
    "# The following line will block for 60 seconds and the console output will be echoed in this notebook\n",
    "# in the cell output. You can unblock earlier by interrupting the Jupyter kernel (menu Krenel -> Interrupt Kernel)\n",
    "streaming_query_read_json.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86b445-8a21-4d9c-930d-ff06839ae762",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_read_json.stop()\n",
    "streaming_query_read_json.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574526d3-5f63-4317-9ba6-a5eba585a1d3",
   "metadata": {},
   "source": [
    "If you want to restart the streaming query with the same JSON files all over again, remove the checkpoint directory `checkpoint_dir_read_json`. Otherwise the query will skip the files that it have read already. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229d450-0e67-480f-8c31-435f7d301b20",
   "metadata": {},
   "source": [
    "### Writitng to Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ce169-83bb-41b4-a8e7-83217b3f692a",
   "metadata": {},
   "source": [
    "The following streaming query writes data read by `df_read_json` from JSON files in `input_directory_of_json_files` directory to files in `output_directory_for_json_files` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ac9ff-3baf-43f7-af55-2eb8676e3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_for_json_files = \"../data_output/streaming_json\"\n",
    "checkpoint_dir_write_json = f\"./spark-streaming-checkpoints-write-json\"\n",
    "\n",
    "streaming_query_write_json = (df_read_json\n",
    "                              .writeStream\n",
    "                              .format(\"json\")\n",
    "                              .option(\"checkpointLocation\", checkpoint_dir_write_json)\n",
    "                              .start(output_directory_for_json_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef8155-b9d7-489a-83a6-e2b0ac8eb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_json.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74252d9d-b4f3-4df7-9e80-98d602dadc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_json.stop()\n",
    "streaming_query_write_json.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3938fec-08a7-4c34-a988-294bb5f82464",
   "metadata": {},
   "source": [
    "### Reading from Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4bc4f-3d4f-41c4-8c72-85b68475f451",
   "metadata": {},
   "source": [
    "Before we can read anything from Kafka, we need to write some data into a topic.  We will use `kafka-time-producer.py` to wirte a stream of timestamps to the `timestamps` Kafka topic.  Then we will read this stream and write it out to console using Spark streaming query.\n",
    "\n",
    "To start producing timestamps into the Kafka topic run the following command from the project root\n",
    "```\n",
    "poetry run python3 bin/kafka-time-producer.py\n",
    "```\n",
    "\n",
    "FYI: `kafka-time-producer.py` generates the key-value pairs and wirtes them to Kafka using Spark, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96434138-db25-4117-ac22-0b04dac4b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_kafka = (spark\n",
    "                 .readStream\n",
    "                 .format(\"kafka\")\n",
    "                 .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "                 .option(\"subscribe\", \"timestamps\")\n",
    "                 .option(\"startingOffsets\", \"earliest\")  # the default for streaming queries is \"latest\"\n",
    "                 .load())\n",
    "# df_read_kafka_transformed = df_read_kafka.withColumns({\"key_string\": F.expr(\"cast(key as string)\"),\n",
    "#                                                        \"value_string\": F.expr(\"cast(value as string)\")})\n",
    "df_read_kafka_transformed = df_read_kafka.withColumns({\"key_string\": F.col(\"key\").cast(\"string\"),\n",
    "                                                       \"value_string\": F.col(\"value\").cast(\"string\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215e708-dfa9-415c-a1bd-e3cd6a48bd79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir_read_kafka = f\"./spark-streaming-checkpoints-read-kafka\"\n",
    "\n",
    "streaming_query_read_kafka = (df_read_kafka_transformed\n",
    "                        .writeStream\n",
    "                        .format(\"console\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .trigger(processingTime=\"1 second\")\n",
    "                        .option(\"checkpointLocation\", checkpoint_dir_read_kafka)\n",
    "                        .start())\n",
    "# The following line will block for 60 seconds and the console output will be echoed in this notebook\n",
    "# in the cell output. You can unblock earlier by interrupting the Jupyter kernel (menu Krenel -> Interrupt Kernel)\n",
    "streaming_query_read_kafka.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b1c93-a2b8-4859-828c-73cb09387185",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_read_kafka.stop()\n",
    "streaming_query_read_kafka.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df323c49-d38a-4a4c-8817-1baa74c99871",
   "metadata": {},
   "source": [
    "### Writing to Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce28789-11a7-43ed-942f-6e2486988805",
   "metadata": {},
   "source": [
    "The following streaming query reads key-value pairs form CSV files in a directory and writes those key-value pairs to a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee04a9-6bad-4db1-9775-4e9809556a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_schema_write_kafka = \"`word` string, `count` long\"\n",
    "\n",
    "df_write_kafka = spark.readStream.format(\"csv\").schema(file_schema_write_kafka).option(\"header\", \"true\").load(\"../data/counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afc3a9-e30f-47e3-a4d0-db55e8423905",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_write_kafka = f\"/tmp/spark-streaming-checkpoints-write-kafka-{uuid1()}\"\n",
    "\n",
    "streaming_query_write_kafka = (df_write_kafka\n",
    "  .selectExpr(\n",
    "    \"cast(word as string) as key\",\n",
    "    \"cast(count as string) as value\")\n",
    "  .writeStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "  .option(\"topic\", \"wordcounts\")\n",
    "  .outputMode(\"update\")\n",
    "  .option(\"checkpointLocation\", checkpoint_dir_write_kafka)\n",
    "  .start())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4cf01-cc5d-48bd-9353-99e7af86c9c4",
   "metadata": {},
   "source": [
    "Check the outputted messages in [AKHQ](http://localhost:8086/ui/docker-kafka-server/topic/wordcounts/data?sort=Oldest&partition=All). If the counts are not written to the Kafka topic, check the terminal where you started the notebook for error logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e0807-675d-406d-93bd-78014ee1def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_kafka.stop()\n",
    "streaming_query_write_kafka.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289469a-1b99-4130-b594-1ad44fdf84dd",
   "metadata": {},
   "source": [
    "### Custom Streaming Sources and Sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d9cb1-bdfb-43f5-be23-d35851d0c897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
