{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a97f35-32ca-4e73-b0a0-5f8763b8c211",
   "metadata": {},
   "source": [
    "# Chapter 8. Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcf7a7-ba94-4f12-a75e-4db2a936afc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from uuid import uuid1\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  # Add Kafka-source library.  The version after \":\" must be the Kafka version that you usew\n",
    "  .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\")\n",
    "  .master(\"local[4]\")\n",
    "  .appName(\"StructuredStreaming\")\n",
    "  .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af97418-4748-4a08-852f-a9f88eb4db6d",
   "metadata": {},
   "source": [
    "## The Fundamentals of a Structured Streaming Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f951e-ef74-498e-b2e1-f67bdf4d7665",
   "metadata": {},
   "source": [
    "For the following streaming query to work, we need a TCP server that will listen at `127.0.0.1:61080` and will be sending text lines.\n",
    "\n",
    "We can use `netcat-openbsd` for this. In a terminal run `nc -lk -s 127.0.0.1 -p 61080` and start typing text lines. Observe the output in this notebook. It should be something like this\n",
    "\n",
    "```\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+----+-----+\n",
    "|word|count|\n",
    "+----+-----+\n",
    "| foo|    1|\n",
    "+----+-----+\n",
    "```\n",
    "\n",
    "To terminate the query interrupt the Jupyter kernel (menu Krenel -> Interrupt Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adc70b-a77c-4ada-80ad-ad0c5d8832f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random checkpoint dirname. Ust it if you want every query to start anew.\n",
    "checkpoint_dir = f\"/tmp/spark-streaming-checkpoints-{uuid1()}\"\n",
    "\n",
    "# Static checkpoint dirname. Use it if you want to restart a stopped query.\n",
    "# checkpoint_dir = f\"./spark-streaming-checkpoints\"\n",
    "\n",
    "# Step 1: Define input sources \n",
    "lines = (spark\n",
    "         .readStream\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"127.0.0.1\")\n",
    "         .option(\"port\", \"61080\")\n",
    "         .load())\n",
    "# Step 2: Transform data\n",
    "words = lines.select(F.explode(F.split(F.col(\"value\"), \"\\\\s\")).alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "# Step 3: Define output sink and output mode\n",
    "writer = (counts\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\"))\n",
    "# Step 4: Specify processing details\n",
    "writer2 = (writer\n",
    "           .trigger(processingTime=\"1 second\")\n",
    "           .option(\"checkpointLocation\", checkpoint_dir))\n",
    "# Step 5: Start the query\n",
    "streaming_query = writer2.start()\n",
    "# The following line will block for 60 seconds and the console output will be echoed in this notebook\n",
    "# in the cell output. You can unblock earlier by interrupting the Jupyter kernel (menu Krenel -> Interrupt Kernel)\n",
    "streaming_query.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566eba32-aba3-4fbd-8e17-81e1670125c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The streaming query is still running. You can still observe the console output\n",
    "# in the terminal in which you started Jupyter.\n",
    "streaming_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7366db9-43f2-44ed-aedd-8d26d516db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()\n",
    "streaming_query.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6be4d-1b6a-4591-a13d-6eb7d8fcc4ff",
   "metadata": {},
   "source": [
    "Now the query is stopped.\n",
    "\n",
    "If you used a static checkpoint dirname, you can restart the query from the point where it left off. To restart the query, reexecute the cell that creates and starts the streaming query (with steps 1 to 5). You may get \"ERROR MicroBatchExecution\" with IndexOutOfBoundsException. In this case rerun the cell one more time.\n",
    "\n",
    "**NOTE:** If you use a static checkpoint dirname and you stopped and restart netcat inbetween, your restarted query may stop accepting input from netcat. In this case you may need a complete reset: stop the query, remove the checkpoint directory manually, restart netcat, and finaly restart the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456856d-6ed7-4372-844b-392a61136063",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28eaf8-613c-4570-9eae-29a39fca3da2",
   "metadata": {},
   "source": [
    "## Streaming Data Sources and Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001d24f-5e8a-41aa-a6ee-35977551eacf",
   "metadata": {},
   "source": [
    "### Reading from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09255a1c-4863-44aa-b009-bf3c085a1d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory_of_json_files = \"../data/streaming_json\"\n",
    "file_schema_read_json = \"`key` integer, `value` string\"\n",
    "\n",
    "df_read_json = (spark\n",
    "           .readStream\n",
    "           .format(\"json\")\n",
    "           .schema(file_schema_read_json)\n",
    "           .load(input_directory_of_json_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c9a0e-1cd1-4cd9-8727-c9c021fde273",
   "metadata": {},
   "source": [
    "After starting the query in the next cell you will see the data from the file `00.json` in the cell output. Create a new file by copying `00.json` to `1.json`:\n",
    "```shell\n",
    "cp data/streaming_json/00.json data/streaming_json/1.json\n",
    "```\n",
    "and you will see the same data output again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19c697-0acf-4d15-8d70-6ebceeb6a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_read_json = f\"./spark-streaming-checkpoints-read-json\"\n",
    "\n",
    "streaming_query_read_json = (df_read_json\n",
    "                        .writeStream\n",
    "                        .format(\"console\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .trigger(processingTime=\"1 second\")\n",
    "                        .option(\"checkpointLocation\", checkpoint_dir_read_json)\n",
    "                        .start())\n",
    "# The following line will block for 60 seconds and the console output will be echoed in this notebook\n",
    "# in the cell output. You can unblock earlier by interrupting the Jupyter kernel (menu Krenel -> Interrupt Kernel)\n",
    "streaming_query_read_json.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86b445-8a21-4d9c-930d-ff06839ae762",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_read_json.stop()\n",
    "streaming_query_read_json.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574526d3-5f63-4317-9ba6-a5eba585a1d3",
   "metadata": {},
   "source": [
    "If you want to restart the streaming query with the same JSON files all over again, remove the checkpoint directory `checkpoint_dir_read_json`. Otherwise the query will skip the files that it have read already. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229d450-0e67-480f-8c31-435f7d301b20",
   "metadata": {},
   "source": [
    "### Writitng to Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ce169-83bb-41b4-a8e7-83217b3f692a",
   "metadata": {},
   "source": [
    "The following streaming query writes data read by `df_read_json` from JSON files in `input_directory_of_json_files` directory to files in `output_directory_for_json_files` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ac9ff-3baf-43f7-af55-2eb8676e3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_for_json_files = \"../data_output/streaming_json\"\n",
    "checkpoint_dir_write_json = f\"./spark-streaming-checkpoints-write-json\"\n",
    "\n",
    "streaming_query_write_json = (df_read_json\n",
    "                              .writeStream\n",
    "                              .format(\"json\")\n",
    "                              .option(\"checkpointLocation\", checkpoint_dir_write_json)\n",
    "                              .start(output_directory_for_json_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef8155-b9d7-489a-83a6-e2b0ac8eb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_json.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74252d9d-b4f3-4df7-9e80-98d602dadc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_json.stop()\n",
    "streaming_query_write_json.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3938fec-08a7-4c34-a988-294bb5f82464",
   "metadata": {},
   "source": [
    "### Reading from Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4bc4f-3d4f-41c4-8c72-85b68475f451",
   "metadata": {},
   "source": [
    "Before we can read anything from Kafka, we need to write some data into a topic.  We will use `kafka-time-producer.py` to wirte a stream of timestamps to the `timestamps` Kafka topic.  Then we will read this stream and write it out to console using Spark streaming query.\n",
    "\n",
    "To start producing timestamps into the Kafka topic run the following command from the project root\n",
    "```shell\n",
    "poetry run python3 bin/kafka-time-producer.py\n",
    "```\n",
    "\n",
    "FYI: `kafka-time-producer.py` generates the key-value pairs and wirtes them to Kafka using Spark, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96434138-db25-4117-ac22-0b04dac4b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_kafka = (spark\n",
    "                 .readStream\n",
    "                 .format(\"kafka\")\n",
    "                 .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "                 .option(\"subscribe\", \"timestamps\")\n",
    "                 .option(\"startingOffsets\", \"earliest\")  # the default for streaming queries is \"latest\"\n",
    "                 .load())\n",
    "# df_read_kafka_transformed = df_read_kafka.withColumns({\"key_string\": F.expr(\"cast(key as string)\"),\n",
    "#                                                        \"value_string\": F.expr(\"cast(value as string)\")})\n",
    "df_read_kafka_transformed = df_read_kafka.withColumns({\"key_string\": F.col(\"key\").cast(\"string\"),\n",
    "                                                       \"value_string\": F.col(\"value\").cast(\"string\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215e708-dfa9-415c-a1bd-e3cd6a48bd79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir_read_kafka = f\"./spark-streaming-checkpoints-read-kafka\"\n",
    "\n",
    "streaming_query_read_kafka = (df_read_kafka_transformed\n",
    "                        .writeStream\n",
    "                        .format(\"console\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .trigger(processingTime=\"1 second\")\n",
    "                        .option(\"checkpointLocation\", checkpoint_dir_read_kafka)\n",
    "                        .start())\n",
    "# The following line will block for 60 seconds and the console output will be echoed in this notebook\n",
    "# in the cell output. You can unblock earlier by interrupting the Jupyter kernel (menu Krenel -> Interrupt Kernel)\n",
    "streaming_query_read_kafka.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b1c93-a2b8-4859-828c-73cb09387185",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_read_kafka.stop()\n",
    "streaming_query_read_kafka.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df323c49-d38a-4a4c-8817-1baa74c99871",
   "metadata": {},
   "source": [
    "### Writing to Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce28789-11a7-43ed-942f-6e2486988805",
   "metadata": {},
   "source": [
    "The following streaming query reads key-value pairs form CSV files in a directory and writes those key-value pairs to a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee04a9-6bad-4db1-9775-4e9809556a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_schema_write_kafka = \"`word` string, `count` long\"\n",
    "\n",
    "df_write_kafka = spark.readStream.format(\"csv\").schema(file_schema_write_kafka).option(\"header\", \"true\").load(\"../data/counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afc3a9-e30f-47e3-a4d0-db55e8423905",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_write_kafka = f\"/tmp/spark-streaming-checkpoints-write-kafka-{uuid1()}\"\n",
    "\n",
    "streaming_query_write_kafka = (df_write_kafka\n",
    "  .selectExpr(\n",
    "    \"cast(word as string) as key\",\n",
    "    \"cast(count as string) as value\")\n",
    "  .writeStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "  .option(\"topic\", \"wordcounts\")\n",
    "  .outputMode(\"update\")\n",
    "  .option(\"checkpointLocation\", checkpoint_dir_write_kafka)\n",
    "  .start())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4cf01-cc5d-48bd-9353-99e7af86c9c4",
   "metadata": {},
   "source": [
    "Check the outputted messages in [AKHQ](http://localhost:8086/ui/docker-kafka-server/topic/wordcounts/data?sort=Oldest&partition=All). If the counts are not written to the Kafka topic, check the terminal where you started the notebook for error logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e0807-675d-406d-93bd-78014ee1def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_kafka.stop()\n",
    "streaming_query_write_kafka.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289469a-1b99-4130-b594-1ad44fdf84dd",
   "metadata": {},
   "source": [
    "### Custom Streaming Sources and Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69b803-1fdb-491e-ad0f-3d98c72d3c06",
   "metadata": {},
   "source": [
    "To demonstrate how to use `foreachBatch()` to write the output of a streaming query to arbitrary storage systems, we will asume that it is only possible to write to a filesystem using a batch connector. We will create a streaming query that will read word counts froma CSV and will write them to a filesystem in JSON format. Instead of using DataStreamWriter, we will use DataFrameWriter in combination with `foreachBatch()` to write streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d9cb1-bdfb-43f5-be23-d35851d0c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_schema_write_kafka = \"`word` string, `count` long\"\n",
    "\n",
    "df_write_anywhere = spark.readStream.format(\"csv\").schema(file_schema_write_kafka).option(\"header\", \"true\").load(\"../data/counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a15dab-45bd-4694-a22f-de8518f71df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_counts_to_filesystem(updated_df, batchId):\n",
    "    updated_df.write.json(path=\"../data_output/streaming_anywhere\", mode=\"append\")\n",
    "\n",
    "checkpoint_dir_write_anywhere = f\"/tmp/spark-streaming-checkpoints-write-anywhere-{uuid1()}\"\n",
    "\n",
    "streaming_query_write_anywhere = (df_write_anywhere\n",
    "                                 .writeStream\n",
    "                                 .foreachBatch(write_counts_to_filesystem)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", checkpoint_dir_write_anywhere)\n",
    "                                 .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58486dc-1998-4f1c-a5c2-6f5b05ae4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_anywhere.stop()\n",
    "streaming_query_write_anywhere.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f7e5-250b-4620-94aa-b7aa77a6ea61",
   "metadata": {},
   "source": [
    "## Stateful Streaming Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811ecca-f97c-4db9-af2a-b5c89ae9edd8",
   "metadata": {},
   "source": [
    "### Aggregations Not Based on Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c0f1f-717d-4b84-839d-31472cb6b768",
   "metadata": {},
   "source": [
    "#### Global aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e47eeb-e66e-4691-9f5d-584f040e77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_aggregations = (spark.readStream\n",
    "                         .format(\"json\")\n",
    "                         .schema(\"`key` integer, `value` string\")\n",
    "                         .load(\"../data/streaming_json/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be9343-dc1e-4a32-9502-2a79661d8cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir_global_aggregations = f\"/tmp/spark-streaming-checkpoints-global-aggregations-{uuid1()}\"\n",
    "\n",
    "streaming_query_global_aggregations = (df_global_aggregations\n",
    "                                       .groupBy()\n",
    "                                       .count()\n",
    "                                       .writeStream\n",
    "                                       .format(\"console\")\n",
    "                                       .outputMode(\"update\")\n",
    "                                       .trigger(processingTime=\"1 second\")\n",
    "                                       .option(\"checkpointLocation\", checkpoint_dir_global_aggregations)\n",
    "                                       .start())\n",
    "streaming_query_global_aggregations.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c837ba7-5701-4f75-8625-61fada102a96",
   "metadata": {},
   "source": [
    "Generate some JSON files for the streaming query to aggregate by running the following loop\n",
    "```shell\n",
    "for I in $(seq 9); do cp data/streaming_json/00.json data/streaming_json/$I.json; sleep 5; done\n",
    "```\n",
    "Clean up the generated files with\n",
    "```shell\n",
    "rm data/streaming_json/?.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfce58-3979-4584-9a61-9dabb89779b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_global_aggregations.stop()\n",
    "streaming_query_global_aggregations.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b71d6f-7155-4a32-a9ae-0cd06f068338",
   "metadata": {},
   "source": [
    "#### Grouped aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706068ba-e278-41ba-847b-cba4f0f54b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_grouped_aggregations = f\"/tmp/spark-streaming-checkpoints-grouped-aggregations-{uuid1()}\"\n",
    "\n",
    "streaming_query_grouped_aggregations = (df_global_aggregations\n",
    "                                       .groupBy(\"value\")\n",
    "                                       .mean(\"key\")\n",
    "                                       .writeStream\n",
    "                                       .format(\"console\")\n",
    "                                       .outputMode(\"complete\")\n",
    "                                       .trigger(processingTime=\"1 second\")\n",
    "                                       .option(\"checkpointLocation\", checkpoint_dir_grouped_aggregations)\n",
    "                                       .start())\n",
    "streaming_query_grouped_aggregations.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803b850-9f46-4a49-b1a5-c8856287d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_grouped_aggregations.stop()\n",
    "streaming_query_grouped_aggregations.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62128fa-9381-436d-8601-53a3be42d986",
   "metadata": {},
   "source": [
    "#### Multiple aggregations computed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75e82a-6e11-439b-86d6-9986e95c993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_multiple_aggregations = f\"/tmp/spark-streaming-checkpoints-multiple-aggregations-{uuid1()}\"\n",
    "\n",
    "streaming_query_multiple_aggregations = (df_global_aggregations\n",
    "                                       .groupBy(\"value\")\n",
    "                                       .agg(F.count(\"*\"),\n",
    "                                            F.mean(\"key\").alias(\"baselineValue\"),\n",
    "                                            F.collect_set(\"key\").alias(\"allValues\"))\n",
    "                                       .writeStream\n",
    "                                       .format(\"console\")\n",
    "                                       .outputMode(\"complete\")\n",
    "                                       .trigger(processingTime=\"1 second\")\n",
    "                                       .option(\"checkpointLocation\", checkpoint_dir_multiple_aggregations)\n",
    "                                       .start())\n",
    "streaming_query_multiple_aggregations.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893aa5c-60ba-40be-997c-b8563e4647cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_multiple_aggregations.stop()\n",
    "streaming_query_multiple_aggregations.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470af77-6707-4ae0-b05f-f26b28c90405",
   "metadata": {},
   "source": [
    "### Aggregations with Event-Time Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37a51e-2fc4-416e-8bbf-8027862353e5",
   "metadata": {},
   "source": [
    "We reproduce the processing of streaming events described in the section \"Handling late data with watermarks\" and illustrated in Figure 8-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751224fd-c11b-4c40-830b-3e844dc1983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# We first crate a list of events as displayed in Figure 8-10.  An event is\n",
    "# identified by its event time, delay, and sensor id.  \n",
    "#\n",
    "# `relative_event_time` are the minutes from the event time of the events\n",
    "# as displayed in Figure 8-10, e.g., the event time of the fist event is 12:07,\n",
    "# so the relative event time is 7.\n",
    "# The relative event time allows us to select an arbitrary starting point,\n",
    "# e.g., 21:00 instead of 12:00.\n",
    "#\n",
    "# `delay` is the difference between the processing time and the event time.\n",
    "#\n",
    "# `sensor_id` and `description` are self-explainatory.\n",
    "Event = namedtuple(\"Event\", [\"relative_event_time\", \"delay\", \"sensor_id\", \"description\"])\n",
    "events_windowing = [\n",
    "    Event(7, 0, \"id1\", \"in window\"),\n",
    "    Event(8, 0, \"id2\", \"in window\"),\n",
    "    Event(9, 4, \"id3\", \"late arival\"),\n",
    "    Event(14, 0, \"id2\", \"in window\"),\n",
    "    Event(15, 2, \"id1\", \"in window\"),\n",
    "    Event(8, 9.5, \"id2\", \"late arival\"),\n",
    "    Event(13, 5, \"id3\", \"late arival\"),\n",
    "    Event(21, -2, \"id2\", \"early arival\"),\n",
    "    Event(4, 18, \"id1\", \"too late\"),\n",
    "    Event(17, 6, \"id3\", \"late arival\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707521ca-be7a-413b-aa6d-466c85b31bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a tombstone to the `events` topic, so that the topic gets created and we can\n",
    "# start a streaming query that will read events from it.\n",
    "(spark.createDataFrame(data=[(None, None)], schema=\"`key` string, `value` string\")\n",
    " .write\n",
    " .format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    " .option(\"topic\", \"events\")\n",
    " .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a35271-73cd-448c-9434-154d1bdf1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataFrame into which events from the Kafka topic will be read.\n",
    "df_windowing = (spark\n",
    "                .readStream\n",
    "                .format(\"kafka\")\n",
    "                .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "                .option(\"subscribe\", \"events\")\n",
    "                .option(\"startingOffsets\", \"earliest\")  # the default for streaming queries is \"latest\"\n",
    "                .load())\n",
    "# Specify a watermark and a grouping with a window in the same way as in the book,\n",
    "# but use seconds for time intervals instead of minutes (for the experiment to run faster).\n",
    "df_windowing_transformed = (df_windowing\n",
    "                            .withColumns({\"sensorId\": F.col(\"key\").cast(\"string\"),\n",
    "                                          \"eventTime\": F.col(\"value\").cast(\"string\").cast(\"timestamp\")})\n",
    "                            .withWatermark(\"eventTime\", \"10 seconds\")\n",
    "                            .groupBy(\"sensorId\", F.window(timeColumn = \"eventTime\",\n",
    "                                                          windowDuration = \"10 seconds\",\n",
    "                                                          slideDuration=\"5 seconds\",\n",
    "                                                          startTime=\"0 seconds\"))\n",
    "                            .count()\n",
    "                            .orderBy([\"window\", \"sensorId\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ed2f5-53c3-41d2-9b4f-af7eb134529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a streaming query that will output the events to the console.\n",
    "checkpoint_dir_windowing = f\"/tmp/spark-streaming-checkpoints-windowing-{uuid1()}\"\n",
    "streaming_query_windowing = (df_windowing_transformed\n",
    "                        .writeStream\n",
    "                        .format(\"console\")\n",
    "                        .outputMode(\"complete\")\n",
    "                        .trigger(processingTime=\"5 seconds\")\n",
    "                        .option(\"checkpointLocation\", checkpoint_dir_windowing)\n",
    "                        .option(\"truncate\", False)\n",
    "                        .start())\n",
    "streaming_query_windowing.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd9eae-353e-437a-a6db-cad9860293f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wait until the next minute starts and seconds equal 0.\n",
    "# This way the seconds in our window ranges will correspond\n",
    "# to minutes in the window ranges in the book.\n",
    "while (now := datetime.now()).second != 0:\n",
    "    sleep(0.01)\n",
    "\n",
    "# Write events to Kafka according to the schedule.\n",
    "total_seconds_passed = 0\n",
    "for event in events_windowing:\n",
    "    wait_seconds = event.relative_event_time + event.delay - total_seconds_passed\n",
    "    event_time = now + timedelta(seconds=event.relative_event_time)\n",
    "    processing_time = event_time + timedelta(seconds=event.delay)\n",
    "    sleep(wait_seconds)\n",
    "    total_seconds_passed += wait_seconds\n",
    "    kafka_message = (event.sensor_id, event_time.isoformat())\n",
    "    # print(event)\n",
    "    # print(kafka_message)\n",
    "    # print(f\"event time: {event_time.isoformat()}\")\n",
    "    # print(f\"processing time: {processing_time.isoformat()}\")\n",
    "    # print(f\"now: {datetime.now().isoformat()}\")\n",
    "    # print()\n",
    "    (spark.createDataFrame(data=[kafka_message], schema=\"`key` string, `value` string\")\n",
    "     .write\n",
    "     .format(\"kafka\")\n",
    "     .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "     .option(\"topic\", \"events\")\n",
    "     .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880e3c6-18a6-4b3b-a190-4819fdb8e419",
   "metadata": {},
   "source": [
    "**NOTICE:**\n",
    "\n",
    "* In the output cell, there should be four batches with aggregated values (as in the Figure 8-10). If you don't see the last batch in the output of the Jupyter cell, check the terminal in which you have started Jupyter. In the terminal all batches should be displayed.\n",
    "\n",
    "* The \"too late\" event `[:04, id1]` may still be aggregated for windows `{:55, :05}` and `{:00, :10}`. This is because a watermark *does not guarantee that data arriving after the threshold is dropped.* It only *guarantees that data arrviving before the threshold is never dropped.* See \"Semantic guarantees with watermarks\" in the book.\n",
    "\n",
    "\n",
    "* There are errors in Figure 8-10, that is why our final DataFrame looks different from that in the book. The event `[:15, id1]` will not be counted in the window `{:05, :15}`, because [the window ends are exclusive](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html#pyspark.sql.functions.window). So, the count for this sensor ID and window will be 1 and not 2 as in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554244b6-57a4-4fd7-969c-fa16f616bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_windowing.stop()\n",
    "streaming_query_windowing.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5834d0-21b1-4f11-8ae7-19b6ae3112d8",
   "metadata": {},
   "source": [
    "To restart the query, first, clear the `events` Kafka topic using [AKHQ](http://localhost:8086/ui/docker-kafka-server/topic/events/data?sort=Oldest&partition=All), then, restart the query by running the cell in which `streaming_query_windowing` is defined and the subsequent cell in which the events are sent to Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076a5b0-ce99-43f7-93cd-0746a12a1b2c",
   "metadata": {},
   "source": [
    "## Streaming Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775c186-d16b-401a-8cfe-0854b5a21c52",
   "metadata": {},
   "source": [
    "### Stream-Static Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7947b1d7-c1e7-418b-aa44-7c9c9c3ad507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the stream-static join example close to that one from the book\n",
    "\n",
    "# A static DataFrame of impressions (i.e., all the ads to be shown)\n",
    "ad_ids = list(range(26))\n",
    "df_impressions = spark.createDataFrame(data=[(i, chr(65+i)) for i in ad_ids],\n",
    "                                       schema=\"`adId` long, `impressionDescription` string\")\n",
    "df_impressions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac5807-c3e5-495e-af59-c5c01f3d83cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create a streaming DataFrame from the rate data source (timestamp-counter pairs)\n",
    "# that we interpret as a stream of clicks.  The column with the counter values\n",
    "# is renamed.  A new column with randm numbers from the range of `ad_ids` is added\n",
    "# as `adId`, and we join the static `df_impressions` against it.\n",
    "#\n",
    "# Rate source docs: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources\n",
    "df_clicks = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "# dd an `adId` column to `df_clicks` with random IDs from the `adId` range. \n",
    "df_impressions_clicks_join = (df_clicks\n",
    "                              .withColumn(\"adId\", F.round(F.rand()*max(ad_ids)).cast(\"integer\"))\n",
    "                              .join(df_impressions, \"adId\")\n",
    "                              .select(\"timestamp\", \"adId\", \"impressionDescription\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8b039-21a2-4fd0-ac76-1fd64f93589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_impressions_clicks_join = f\"/tmp/spark-streaming-checkpoints-impressions-clicks-join-{uuid1()}\"\n",
    "\n",
    "straming_query_impressions_clicks_join = (df_impressions_clicks_join\n",
    "                                          .writeStream\n",
    "                                          .format(\"console\")\n",
    "                                          .option(\"truncate\", False)\n",
    "                                          .option(\"checkpointLocation\", checkpoint_dir_impressions_clicks_join)\n",
    "                                          .outputMode(\"append\")\n",
    "                                          .trigger(processingTime=\"2 seconds\")\n",
    "                                          .start())\n",
    "straming_query_impressions_clicks_join.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c60b80-1512-4d46-8444-b8a0ac95eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "straming_query_impressions_clicks_join.stop()\n",
    "straming_query_impressions_clicks_join.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02021319-80dd-45fb-a7d1-696139b60624",
   "metadata": {},
   "source": [
    "### Stream-Stream Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1d3b4-a515-4aea-9009-ff7b0b7f38c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ad_ids = list(range(3))\n",
    "\n",
    "impressions_with_watermark = (spark\n",
    "                              .readStream\n",
    "                              .format(\"rate\")\n",
    "                              .option(\"rowsPerSecond\", 1)\n",
    "                              .load()\n",
    "                              .withColumn(\"impressionAdId\", F.round(F.rand()*max(ad_ids)).cast(\"integer\"))\n",
    "                              .select(\"impressionAdId\", F.col(\"timestamp\").alias(\"impressionTime\"))\n",
    "                              .withWatermark(\"impressionTime\", \"2 hours\"))\n",
    "clicks_with_watermark = (spark\n",
    "                              .readStream\n",
    "                              .format(\"rate\")\n",
    "                              .option(\"rowsPerSecond\", 2)\n",
    "                              .option(\"rampUpTime\", \"5s\")  # 5 seconds ramp up before the generating speed becomes as specified\n",
    "                              .load()\n",
    "                              .withColumn(\"clickAdId\", F.round(F.rand()*max(ad_ids)).cast(\"integer\"))\n",
    "                              .select(\"clickAdId\", F.col(\"timestamp\").alias(\"clickTime\"))\n",
    "                              .withWatermark(\"clickTime\", \"2 hours\"))\n",
    "\n",
    "impressions_clicks_join = impressions_with_watermark.join(clicks_with_watermark,\n",
    "                                                          on=F.expr(\"\"\"\n",
    "                                                              clickAdId = impressionAdId AND \n",
    "                                                              clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour\"\"\"),\n",
    "                                                          how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b01df6b-6237-4612-9463-189523139452",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_impressions_clicks_stream_stream_join = f\"/tmp/spark-streaming-checkpoints-impressions-clicks-stream-stream-join-{uuid1()}\"\n",
    "\n",
    "straming_query_impressions_clicks_stream_stream_join = (impressions_clicks_join\n",
    "                                          .writeStream\n",
    "                                          .format(\"console\")\n",
    "                                          .option(\"truncate\", False)\n",
    "                                          .option(\"checkpointLocation\", checkpoint_dir_impressions_clicks_stream_stream_join)\n",
    "                                          .outputMode(\"append\")\n",
    "                                          .trigger(processingTime=\"2 seconds\")\n",
    "                                          .start())\n",
    "straming_query_impressions_clicks_stream_stream_join.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a8ce2-ed3e-4302-b74a-ddfb9c8c901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "straming_query_impressions_clicks_stream_stream_join.stop()\n",
    "straming_query_impressions_clicks_stream_stream_join.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf505e-12a2-4b9f-8472-d12e16c1ac2e",
   "metadata": {},
   "source": [
    "## Arbitrary Stateful Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b37678-ba65-457e-ab61-41ac94e7ada0",
   "metadata": {},
   "source": [
    "As of Spark 3.0, `mapGroupsWithState(`) and its more flexible counterpart `flatMapGroupsWithState()`, which allow to implement arbitrary stateful computation, are only available in Scala and Java.\n",
    "\n",
    "As of Spark 3.4, `DataFrame.groupby.applyInPandasWithState` was added and brings arbitrary stateful computations to PySpark in the similar way as `flatMapGroupsWithState()` does it for Scala.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fac93-a04f-4183-9b42-d9dbf3ddaa08",
   "metadata": {},
   "source": [
    "### Word Count Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f204b75-6335-48c0-92ea-e87ff2ec15a9",
   "metadata": {},
   "source": [
    "The following example is take from the article [Python Arbitrary Stateful Processing in Structured Streaming](https://web.archive.org/web/20230421231017/https://www.databricks.com/blog/2022/10/18/python-arbitrary-stateful-processing-structured-streaming.html).  The example counts words that are streamed from text files.  The counting is done in session windows with a session window per word and its count.  A session window is closed if there is no corresponding word in the stream for 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f047b-158f-4ca9-9067-c098f01b244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import math\n",
    "import time\n",
    "from typing import Tuple, Iterator\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "# My text files containing words will be created in this directory later\n",
    "# after cleaning 'words_dir' directory up in case you already ran this\n",
    "# example below.\n",
    "words_dir = Path(\"../data\") / \"words_dir\"\n",
    "shutil.rmtree(words_dir, ignore_errors=True)\n",
    "words_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9e01f-3f8c-4cfd-933c-f6f51bf5898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, start a streaming query that ingests 'words_dir' directory.\n",
    "# Every time when there are new text files arriving here, we will process them.\n",
    "lines = spark.readStream.text(Path(words_dir).resolve().as_uri())\n",
    "\n",
    "# Split the lines into words.\n",
    "events = lines.select(F.explode(F.split(lines.value, \" \")).alias(\"session\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b43450c-9f14-470c-a163-884fa2d80887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the session window logic with DataFrame.groupby.applyInPandasWithState()\n",
    "\n",
    "def func(\n",
    "    key: Tuple[str], pdfs: Iterator[pd.DataFrame], state: GroupState\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    if state.hasTimedOut:\n",
    "        (word,) = key\n",
    "        (count,) = state.get\n",
    "        state.remove()\n",
    "        yield pd.DataFrame({\"session\": [word], \"count\": [count]})\n",
    "    else:\n",
    "        # Aggregate the number of words.\n",
    "        count = sum(map(lambda pdf: len(pdf), pdfs))\n",
    "        if state.exists:\n",
    "            (old_count,) = state.get\n",
    "            count += old_count\n",
    "        state.update((count,))\n",
    "        # Set the timeout as 10 seconds.\n",
    "        state.setTimeoutDuration(10000)\n",
    "        yield pd.DataFrame()\n",
    "\n",
    "\n",
    "# Group the data by word, and compute the count of each group\n",
    "output_schema = \"session STRING, count LONG\"\n",
    "state_schema = \"count LONG\"\n",
    "sessions = events.groupBy(events[\"session\"]).applyInPandasWithState(\n",
    "    func,\n",
    "    output_schema,\n",
    "    state_schema,\n",
    "    \"append\",\n",
    "    GroupStateTimeout.ProcessingTimeTimeout,\n",
    ")\n",
    "\n",
    "# Start running the query that prints the windowed word counts to the console.\n",
    "query = sessions.writeStream.foreachBatch(lambda df, _: df.show()).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b19d3a-b8b7-48f3-acc2-28b83c715264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will write words to be processed in a streaming manner\n",
    "# Write 1 banana, 2 grapes, and 3 apples.\n",
    "with open(words_dir / \"words.txt\", \"w\") as f:\n",
    "    _ = f.write(\"banana grape apple\\n\")\n",
    "    _ = f.write(\"banana apple apple\\n\")\n",
    "\n",
    "# Write 3 bananas and 3 grapes every second for 10 seconds.\n",
    "for i in range(10):\n",
    "    time.sleep(1)\n",
    "    with open(words_dir / f\"words_{i}.txt\", \"w\") as f:\n",
    "        _ = f.write(\"banana banana banana\\n\")\n",
    "        _ = f.write(\"grape grape grape\\n\")\n",
    "\n",
    "# Wait enough for the query to finish the input.\n",
    "time.sleep(90)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e817c5c-b9eb-4e14-bd21-2892830f6674",
   "metadata": {},
   "source": [
    "### flatMapGroupsWithState() Example from the Book Reimplemented in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f4d15-84da-404d-8981-30ce27f855e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "max_user_id = 10\n",
    "\n",
    "@F.pandas_udf(StringType())\n",
    "def random_user_actions(a: pd.Series) -> pd.Series:\n",
    "    user_actions = [\"signing in\", \"searching\", \"browsing the home page\", \"browsing products\", \"buying\"]\n",
    "    return pd.Series(np.random.choice(user_actions, size=len(a)))\n",
    "\n",
    "# Produce a stream of user states with user IDs and event timestamps like this\n",
    "# +--------------------+------+----------+\n",
    "# |      eventTimestamp|userId| userState|\n",
    "# +--------------------+------+----------+\n",
    "# |2023-07-22 23:06:...|     4|signing in|\n",
    "# +--------------------+------+----------+\n",
    "user_actions = (spark\n",
    "                .readStream\n",
    "                .format(\"rate\")\n",
    "                .option(\"rowsPerSecond\", 1)\n",
    "                .load()\n",
    "                .withColumn(\"userId\", F.round(F.rand() * max_user_id).cast(\"integer\"))\n",
    "                .withColumn(\"userState\", random_user_actions(F.col(\"userId\")))\n",
    "                .withColumnRenamed(\"timestamp\", \"eventTimestamp\")\n",
    "                .select(\"eventTimestamp\", \"userId\", \"userState\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4b002-10b8-4396-8494-31ffb993817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_status(\n",
    "    key: Tuple[str], pdfs: Iterator[pd.DataFrame], state: GroupState\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    (user_id, ) = key\n",
    "    if not state.hasTimedOut:\n",
    "        if state.exists:\n",
    "            # We do not do anything with the previous state for the user_id\n",
    "            # in this example, because we only need to replace it with the new state.\n",
    "            # The previous state would be needed if had in it a counter\n",
    "            # or an aggregator.\n",
    "            (previous_user_id, previous_is_active) = state.get\n",
    "        # We have got an action from a user, so the user is still active.\n",
    "        is_active = True\n",
    "        state.update((user_id, is_active))\n",
    "        # Set the timeout timestamp to the current watermark + 30 seconds (i.e., 30 * 1000 ms)\n",
    "        state.setTimeoutTimestamp(state.getCurrentWatermarkMs() + (30 * 1000))\n",
    "    else:\n",
    "        is_active = False\n",
    "        state.remove()\n",
    "    yield pd.DataFrame({\"userId\": [user_id], \"is_active\": [is_active]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516141b1-348c-4b33-86d0-6239794596ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_schema = \"userId INTEGER, is_active BOOLEAN\"\n",
    "state_schema = \"userId INTEGER, is_active BOOLEAN\"\n",
    "latest_statuses = (user_actions\n",
    "                   .withWatermark(\"eventTimestamp\", \"10 seconds\")\n",
    "                   .groupBy(user_actions.userId)\n",
    "                   .applyInPandasWithState(\n",
    "                       update_user_status,\n",
    "                       output_schema,\n",
    "                       state_schema,\n",
    "                       \"append\",\n",
    "                       GroupStateTimeout.EventTimeTimeout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c56f3-9920-44bd-a383-dbb91e770351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not specify a checkpoint location, so a temporary checkpoint directory will be created in `/tmp`\n",
    "streaming_query_latest_statuses = (latest_statuses\n",
    "                                   .writeStream\n",
    "                                   .format(\"console\")\n",
    "                                   .option(\"truncate\", False)\n",
    "                                   .trigger(processingTime=\"10 seconds\")\n",
    "                                   .outputMode(\"append\")\n",
    "                                   .start())\n",
    "streaming_query_latest_statuses.awaitTermination(180)\n",
    "streaming_query_latest_statuses.stop()\n",
    "streaming_query_latest_statuses.status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
