{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a97f35-32ca-4e73-b0a0-5f8763b8c211",
   "metadata": {},
   "source": [
    "# Chapter 8. Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcf7a7-ba94-4f12-a75e-4db2a936afc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from uuid import uuid1\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  # Add Kafka-source library.  The version after \":\" must be the Kafka version that you usew\n",
    "  .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\")\n",
    "  .master(\"local[4]\")\n",
    "  .appName(\"StructuredStreaming\")\n",
    "  .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af97418-4748-4a08-852f-a9f88eb4db6d",
   "metadata": {},
   "source": [
    "## The Fundamentals of a Structured Streaming Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f951e-ef74-498e-b2e1-f67bdf4d7665",
   "metadata": {},
   "source": [
    "For the following streaming query to work, we need a TCP server that will listen at `127.0.0.1:61080` and will be sending text lines.\n",
    "\n",
    "We can use `netcat-openbsd` for this. In a terminal run `nc -lk -s 127.0.0.1 -p 61080` and start typing text lines. Observe the output in this notebook. It should be something like this\n",
    "\n",
    "```\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+----+-----+\n",
    "|word|count|\n",
    "+----+-----+\n",
    "| foo|    1|\n",
    "+----+-----+\n",
    "```\n",
    "\n",
    "To terminate the query interrupt the Jupyter kernel (menu Krenel -> Interrupt Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adc70b-a77c-4ada-80ad-ad0c5d8832f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random checkpoint dirname. Ust it if you want every query to start anew.\n",
    "checkpoint_dir = f\"/tmp/spark-streaming-checkpoints-{uuid1()}\"\n",
    "\n",
    "# Static checkpoint dirname. Use it if you want to restart a stopped query.\n",
    "# checkpoint_dir = f\"./spark-streaming-checkpoints\"\n",
    "\n",
    "# Step 1: Define input sources \n",
    "lines = (spark\n",
    "         .readStream\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"127.0.0.1\")\n",
    "         .option(\"port\", \"61080\")\n",
    "         .load())\n",
    "# Step 2: Transform data\n",
    "words = lines.select(F.explode(F.split(F.col(\"value\"), \"\\\\s\")).alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "# Step 3: Define output sink and output mode\n",
    "writer = (counts\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\"))\n",
    "# Step 4: Specify processing details\n",
    "writer2 = (writer\n",
    "           .trigger(processingTime=\"1 second\")\n",
    "           .option(\"checkpointLocation\", checkpoint_dir))\n",
    "# Step 5: Start the query\n",
    "streaming_query = writer2.start()\n",
    "# The following line will block for 60 seconds and the console output will be echoed in this notebook\n",
    "# in the cell output. You can unblock earlier by interrupting the Jupyter kernel (menu Krenel -> Interrupt Kernel)\n",
    "streaming_query.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566eba32-aba3-4fbd-8e17-81e1670125c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The streaming query is still running. You can still observe the console output\n",
    "# in the terminal in which you started Jupyter.\n",
    "streaming_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7366db9-43f2-44ed-aedd-8d26d516db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()\n",
    "streaming_query.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6be4d-1b6a-4591-a13d-6eb7d8fcc4ff",
   "metadata": {},
   "source": [
    "Now the query is stopped.\n",
    "\n",
    "If you used a static checkpoint dirname, you can restart the query from the point where it left off. To restart the query, reexecute the cell that creates and starts the streaming query (with steps 1 to 5). You may get \"ERROR MicroBatchExecution\" with IndexOutOfBoundsException. In this case rerun the cell one more time.\n",
    "\n",
    "**NOTE:** If you use a static checkpoint dirname and you stopped and restart netcat inbetween, your restarted query may stop accepting input from netcat. In this case you may need a complete reset: stop the query, remove the checkpoint directory manually, restart netcat, and finaly restart the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456856d-6ed7-4372-844b-392a61136063",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28eaf8-613c-4570-9eae-29a39fca3da2",
   "metadata": {},
   "source": [
    "## Streaming Data Sources and Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001d24f-5e8a-41aa-a6ee-35977551eacf",
   "metadata": {},
   "source": [
    "### Reading from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09255a1c-4863-44aa-b009-bf3c085a1d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory_of_json_files = \"../data/streaming_json\"\n",
    "file_schema_read_json = \"`key` integer, `value` string\"\n",
    "\n",
    "df_read_json = (spark\n",
    "           .readStream\n",
    "           .format(\"json\")\n",
    "           .schema(file_schema_read_json)\n",
    "           .load(input_directory_of_json_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c9a0e-1cd1-4cd9-8727-c9c021fde273",
   "metadata": {},
   "source": [
    "After starting the query in the next cell you will see the data from the file `00.json` in the cell output. Create a new file by copying `00.json` to `1.json`:\n",
    "```shell\n",
    "cp data/streaming_json/00.json data/streaming_json/1.json\n",
    "```\n",
    "and you will see the same data output again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19c697-0acf-4d15-8d70-6ebceeb6a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_read_json = f\"./spark-streaming-checkpoints-read-json\"\n",
    "\n",
    "streaming_query_read_json = (df_read_json\n",
    "                        .writeStream\n",
    "                        .format(\"console\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .trigger(processingTime=\"1 second\")\n",
    "                        .option(\"checkpointLocation\", checkpoint_dir_read_json)\n",
    "                        .start())\n",
    "# The following line will block for 60 seconds and the console output will be echoed in this notebook\n",
    "# in the cell output. You can unblock earlier by interrupting the Jupyter kernel (menu Krenel -> Interrupt Kernel)\n",
    "streaming_query_read_json.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86b445-8a21-4d9c-930d-ff06839ae762",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_read_json.stop()\n",
    "streaming_query_read_json.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574526d3-5f63-4317-9ba6-a5eba585a1d3",
   "metadata": {},
   "source": [
    "If you want to restart the streaming query with the same JSON files all over again, remove the checkpoint directory `checkpoint_dir_read_json`. Otherwise the query will skip the files that it have read already. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229d450-0e67-480f-8c31-435f7d301b20",
   "metadata": {},
   "source": [
    "### Writitng to Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ce169-83bb-41b4-a8e7-83217b3f692a",
   "metadata": {},
   "source": [
    "The following streaming query writes data read by `df_read_json` from JSON files in `input_directory_of_json_files` directory to files in `output_directory_for_json_files` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ac9ff-3baf-43f7-af55-2eb8676e3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_for_json_files = \"../data_output/streaming_json\"\n",
    "checkpoint_dir_write_json = f\"./spark-streaming-checkpoints-write-json\"\n",
    "\n",
    "streaming_query_write_json = (df_read_json\n",
    "                              .writeStream\n",
    "                              .format(\"json\")\n",
    "                              .option(\"checkpointLocation\", checkpoint_dir_write_json)\n",
    "                              .start(output_directory_for_json_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef8155-b9d7-489a-83a6-e2b0ac8eb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_json.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74252d9d-b4f3-4df7-9e80-98d602dadc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_json.stop()\n",
    "streaming_query_write_json.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3938fec-08a7-4c34-a988-294bb5f82464",
   "metadata": {},
   "source": [
    "### Reading from Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4bc4f-3d4f-41c4-8c72-85b68475f451",
   "metadata": {},
   "source": [
    "Before we can read anything from Kafka, we need to write some data into a topic.  We will use `kafka-time-producer.py` to wirte a stream of timestamps to the `timestamps` Kafka topic.  Then we will read this stream and write it out to console using Spark streaming query.\n",
    "\n",
    "To start producing timestamps into the Kafka topic run the following command from the project root\n",
    "```shell\n",
    "poetry run python3 bin/kafka-time-producer.py\n",
    "```\n",
    "\n",
    "FYI: `kafka-time-producer.py` generates the key-value pairs and wirtes them to Kafka using Spark, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96434138-db25-4117-ac22-0b04dac4b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_kafka = (spark\n",
    "                 .readStream\n",
    "                 .format(\"kafka\")\n",
    "                 .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "                 .option(\"subscribe\", \"timestamps\")\n",
    "                 .option(\"startingOffsets\", \"earliest\")  # the default for streaming queries is \"latest\"\n",
    "                 .load())\n",
    "# df_read_kafka_transformed = df_read_kafka.withColumns({\"key_string\": F.expr(\"cast(key as string)\"),\n",
    "#                                                        \"value_string\": F.expr(\"cast(value as string)\")})\n",
    "df_read_kafka_transformed = df_read_kafka.withColumns({\"key_string\": F.col(\"key\").cast(\"string\"),\n",
    "                                                       \"value_string\": F.col(\"value\").cast(\"string\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215e708-dfa9-415c-a1bd-e3cd6a48bd79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir_read_kafka = f\"./spark-streaming-checkpoints-read-kafka\"\n",
    "\n",
    "streaming_query_read_kafka = (df_read_kafka_transformed\n",
    "                        .writeStream\n",
    "                        .format(\"console\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .trigger(processingTime=\"1 second\")\n",
    "                        .option(\"checkpointLocation\", checkpoint_dir_read_kafka)\n",
    "                        .start())\n",
    "# The following line will block for 60 seconds and the console output will be echoed in this notebook\n",
    "# in the cell output. You can unblock earlier by interrupting the Jupyter kernel (menu Krenel -> Interrupt Kernel)\n",
    "streaming_query_read_kafka.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b1c93-a2b8-4859-828c-73cb09387185",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_read_kafka.stop()\n",
    "streaming_query_read_kafka.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df323c49-d38a-4a4c-8817-1baa74c99871",
   "metadata": {},
   "source": [
    "### Writing to Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce28789-11a7-43ed-942f-6e2486988805",
   "metadata": {},
   "source": [
    "The following streaming query reads key-value pairs form CSV files in a directory and writes those key-value pairs to a Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee04a9-6bad-4db1-9775-4e9809556a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_schema_write_kafka = \"`word` string, `count` long\"\n",
    "\n",
    "df_write_kafka = spark.readStream.format(\"csv\").schema(file_schema_write_kafka).option(\"header\", \"true\").load(\"../data/counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afc3a9-e30f-47e3-a4d0-db55e8423905",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_write_kafka = f\"/tmp/spark-streaming-checkpoints-write-kafka-{uuid1()}\"\n",
    "\n",
    "streaming_query_write_kafka = (df_write_kafka\n",
    "  .selectExpr(\n",
    "    \"cast(word as string) as key\",\n",
    "    \"cast(count as string) as value\")\n",
    "  .writeStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "  .option(\"topic\", \"wordcounts\")\n",
    "  .outputMode(\"update\")\n",
    "  .option(\"checkpointLocation\", checkpoint_dir_write_kafka)\n",
    "  .start())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4cf01-cc5d-48bd-9353-99e7af86c9c4",
   "metadata": {},
   "source": [
    "Check the outputted messages in [AKHQ](http://localhost:8086/ui/docker-kafka-server/topic/wordcounts/data?sort=Oldest&partition=All). If the counts are not written to the Kafka topic, check the terminal where you started the notebook for error logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e0807-675d-406d-93bd-78014ee1def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_kafka.stop()\n",
    "streaming_query_write_kafka.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289469a-1b99-4130-b594-1ad44fdf84dd",
   "metadata": {},
   "source": [
    "### Custom Streaming Sources and Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69b803-1fdb-491e-ad0f-3d98c72d3c06",
   "metadata": {},
   "source": [
    "To demonstrate how to use `foreachBatch()` to write the output of a streaming query to arbitrary storage systems, we will asume that it is only possible to write to a filesystem using a batch connector. We will create a streaming query that will read word counts froma CSV and will write them to a filesystem in JSON format. Instead of using DataStreamWriter, we will use DataFrameWriter in combination with `foreachBatch()` to write streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d9cb1-bdfb-43f5-be23-d35851d0c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_schema_write_kafka = \"`word` string, `count` long\"\n",
    "\n",
    "df_write_anywhere = spark.readStream.format(\"csv\").schema(file_schema_write_kafka).option(\"header\", \"true\").load(\"../data/counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a15dab-45bd-4694-a22f-de8518f71df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_counts_to_filesystem(updated_df, batchId):\n",
    "    updated_df.write.json(path=\"../data_output/streaming_anywhere\", mode=\"append\")\n",
    "\n",
    "checkpoint_dir_write_anywhere = f\"/tmp/spark-streaming-checkpoints-write-anywhere-{uuid1()}\"\n",
    "\n",
    "streaming_query_write_anywhere = (df_write_anywhere\n",
    "                                 .writeStream\n",
    "                                 .foreachBatch(write_counts_to_filesystem)\n",
    "                                 .outputMode(\"update\")\n",
    "                                 .option(\"checkpointLocation\", checkpoint_dir_write_anywhere)\n",
    "                                 .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58486dc-1998-4f1c-a5c2-6f5b05ae4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_write_anywhere.stop()\n",
    "streaming_query_write_anywhere.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f7e5-250b-4620-94aa-b7aa77a6ea61",
   "metadata": {},
   "source": [
    "## Stateful Streaming Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811ecca-f97c-4db9-af2a-b5c89ae9edd8",
   "metadata": {},
   "source": [
    "### Aggregations Not Based on Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c0f1f-717d-4b84-839d-31472cb6b768",
   "metadata": {},
   "source": [
    "#### Global aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e47eeb-e66e-4691-9f5d-584f040e77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_aggregations = (spark.readStream\n",
    "                         .format(\"json\")\n",
    "                         .schema(\"`key` integer, `value` string\")\n",
    "                         .load(\"../data/streaming_json/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be9343-dc1e-4a32-9502-2a79661d8cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir_global_aggregations = f\"/tmp/spark-streaming-checkpoints-global-aggregations-{uuid1()}\"\n",
    "\n",
    "streaming_query_global_aggregations = (df_global_aggregations\n",
    "                                       .groupBy()\n",
    "                                       .count()\n",
    "                                       .writeStream\n",
    "                                       .format(\"console\")\n",
    "                                       .outputMode(\"update\")\n",
    "                                       .trigger(processingTime=\"1 second\")\n",
    "                                       .option(\"checkpointLocation\", checkpoint_dir_global_aggregations)\n",
    "                                       .start())\n",
    "streaming_query_global_aggregations.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c837ba7-5701-4f75-8625-61fada102a96",
   "metadata": {},
   "source": [
    "Generate some JSON files for the streaming query to aggregate by running the following loop\n",
    "```shell\n",
    "for I in $(seq 9); do cp data/streaming_json/00.json data/streaming_json/$I.json; sleep 5; done\n",
    "```\n",
    "Clean up the generated files with\n",
    "```shell\n",
    "rm data/streaming_json/?.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfce58-3979-4584-9a61-9dabb89779b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_global_aggregations.stop()\n",
    "streaming_query_global_aggregations.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b71d6f-7155-4a32-a9ae-0cd06f068338",
   "metadata": {},
   "source": [
    "#### Grouped aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706068ba-e278-41ba-847b-cba4f0f54b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_grouped_aggregations = f\"/tmp/spark-streaming-checkpoints-grouped-aggregations-{uuid1()}\"\n",
    "\n",
    "streaming_query_grouped_aggregations = (df_global_aggregations\n",
    "                                       .groupBy(\"value\")\n",
    "                                       .mean(\"key\")\n",
    "                                       .writeStream\n",
    "                                       .format(\"console\")\n",
    "                                       .outputMode(\"complete\")\n",
    "                                       .trigger(processingTime=\"1 second\")\n",
    "                                       .option(\"checkpointLocation\", checkpoint_dir_grouped_aggregations)\n",
    "                                       .start())\n",
    "streaming_query_grouped_aggregations.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803b850-9f46-4a49-b1a5-c8856287d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_grouped_aggregations.stop()\n",
    "streaming_query_grouped_aggregations.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62128fa-9381-436d-8601-53a3be42d986",
   "metadata": {},
   "source": [
    "#### Multiple aggregations computed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75e82a-6e11-439b-86d6-9986e95c993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_multiple_aggregations = f\"/tmp/spark-streaming-checkpoints-multiple-aggregations-{uuid1()}\"\n",
    "\n",
    "streaming_query_multiple_aggregations = (df_global_aggregations\n",
    "                                       .groupBy(\"value\")\n",
    "                                       .agg(F.count(\"*\"),\n",
    "                                            F.mean(\"key\").alias(\"baselineValue\"),\n",
    "                                            F.collect_set(\"key\").alias(\"allValues\"))\n",
    "                                       .writeStream\n",
    "                                       .format(\"console\")\n",
    "                                       .outputMode(\"complete\")\n",
    "                                       .trigger(processingTime=\"1 second\")\n",
    "                                       .option(\"checkpointLocation\", checkpoint_dir_multiple_aggregations)\n",
    "                                       .start())\n",
    "streaming_query_multiple_aggregations.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893aa5c-60ba-40be-997c-b8563e4647cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_multiple_aggregations.stop()\n",
    "streaming_query_multiple_aggregations.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470af77-6707-4ae0-b05f-f26b28c90405",
   "metadata": {},
   "source": [
    "### Aggregations with Event-Time Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37a51e-2fc4-416e-8bbf-8027862353e5",
   "metadata": {},
   "source": [
    "We reproduce the processing of streaming events described in the section \"Handling late data with watermarks\" and illustrated in Figure 8-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751224fd-c11b-4c40-830b-3e844dc1983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# We first crate a list of events as displayed in Figure 8-10.  An event is\n",
    "# identified by its event time, delay, and sensor id.  \n",
    "#\n",
    "# `relative_event_time` are the minutes from the event time of the events\n",
    "# as displayed in Figure 8-10, e.g., the event time of the fist event is 12:07,\n",
    "# so the relative event time is 7.\n",
    "# The relative event time allows us to select an arbitrary starting point,\n",
    "# e.g., 21:00 instead of 12:00.\n",
    "#\n",
    "# `delay` is the difference between the processing time and the event time.\n",
    "#\n",
    "# `sensor_id` and `description` are self-explainatory.\n",
    "Event = namedtuple(\"Event\", [\"relative_event_time\", \"delay\", \"sensor_id\", \"description\"])\n",
    "events_windowing = [\n",
    "    Event(7, 0, \"id1\", \"in window\"),\n",
    "    Event(8, 0, \"id2\", \"in window\"),\n",
    "    Event(9, 4, \"id3\", \"late arival\"),\n",
    "    Event(14, 0, \"id2\", \"in window\"),\n",
    "    Event(15, 2, \"id1\", \"in window\"),\n",
    "    Event(8, 9.5, \"id2\", \"late arival\"),\n",
    "    Event(13, 5, \"id3\", \"late arival\"),\n",
    "    Event(21, -2, \"id2\", \"early arival\"),\n",
    "    Event(4, 18, \"id1\", \"too late\"),\n",
    "    Event(17, 6, \"id3\", \"late arival\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707521ca-be7a-413b-aa6d-466c85b31bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a tombstone to the `events` topic, so that the topic gets created and we can\n",
    "# start a streaming query that will read events from it.\n",
    "(spark.createDataFrame(data=[(None, None)], schema=\"`key` string, `value` string\")\n",
    " .write\n",
    " .format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    " .option(\"topic\", \"events\")\n",
    " .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a35271-73cd-448c-9434-154d1bdf1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataFrame into which events from the Kafka topic will be read.\n",
    "df_windowing = (spark\n",
    "                .readStream\n",
    "                .format(\"kafka\")\n",
    "                .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "                .option(\"subscribe\", \"events\")\n",
    "                .option(\"startingOffsets\", \"earliest\")  # the default for streaming queries is \"latest\"\n",
    "                .load())\n",
    "# Specify a watermark and a grouping with a window in the same way as in the book,\n",
    "# but use seconds for time intervals instead of minutes (for the experiment to run faster).\n",
    "df_windowing_transformed = (df_windowing\n",
    "                            .withColumns({\"sensorId\": F.col(\"key\").cast(\"string\"),\n",
    "                                          \"eventTime\": F.col(\"value\").cast(\"string\").cast(\"timestamp\")})\n",
    "                            .withWatermark(\"eventTime\", \"10 seconds\")\n",
    "                            .groupBy(\"sensorId\", F.window(timeColumn = \"eventTime\",\n",
    "                                                          windowDuration = \"10 seconds\",\n",
    "                                                          slideDuration=\"5 seconds\",\n",
    "                                                          startTime=\"0 seconds\"))\n",
    "                            .count()\n",
    "                            .orderBy([\"window\", \"sensorId\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ed2f5-53c3-41d2-9b4f-af7eb134529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a streaming query that will output the events to the console.\n",
    "checkpoint_dir_windowing = f\"/tmp/spark-streaming-checkpoints-windowing-{uuid1()}\"\n",
    "streaming_query_windowing = (df_windowing_transformed\n",
    "                        .writeStream\n",
    "                        .format(\"console\")\n",
    "                        .outputMode(\"complete\")\n",
    "                        .trigger(processingTime=\"5 seconds\")\n",
    "                        .option(\"checkpointLocation\", checkpoint_dir_windowing)\n",
    "                        .option(\"truncate\", False)\n",
    "                        .start())\n",
    "streaming_query_windowing.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd9eae-353e-437a-a6db-cad9860293f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wait until the next minute starts and seconds equal 0.\n",
    "# This way the seconds in our window ranges will correspond\n",
    "# to minutes in the window ranges in the book.\n",
    "while (now := datetime.now()).second != 0:\n",
    "    sleep(0.01)\n",
    "\n",
    "# Write events to Kafka according to the schedule.\n",
    "total_seconds_passed = 0\n",
    "for event in events_windowing:\n",
    "    wait_seconds = event.relative_event_time + event.delay - total_seconds_passed\n",
    "    event_time = now + timedelta(seconds=event.relative_event_time)\n",
    "    processing_time = event_time + timedelta(seconds=event.delay)\n",
    "    sleep(wait_seconds)\n",
    "    total_seconds_passed += wait_seconds\n",
    "    kafka_message = (event.sensor_id, event_time.isoformat())\n",
    "    # print(event)\n",
    "    # print(kafka_message)\n",
    "    # print(f\"event time: {event_time.isoformat()}\")\n",
    "    # print(f\"processing time: {processing_time.isoformat()}\")\n",
    "    # print(f\"now: {datetime.now().isoformat()}\")\n",
    "    # print()\n",
    "    (spark.createDataFrame(data=[kafka_message], schema=\"`key` string, `value` string\")\n",
    "     .write\n",
    "     .format(\"kafka\")\n",
    "     .option(\"kafka.bootstrap.servers\", \"localhost:9093,localhost:9094,localhost:9095\")\n",
    "     .option(\"topic\", \"events\")\n",
    "     .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880e3c6-18a6-4b3b-a190-4819fdb8e419",
   "metadata": {},
   "source": [
    "**NOTICE:**\n",
    "\n",
    "* In the output cell, there should be four batches with aggregated values (as in the Figure 8-10). If you don't see the last batch in the output of the Jupyter cell, check the terminal in which you have started Jupyter. In the terminal all batches should be displayed.\n",
    "\n",
    "* The \"too late\" event `[:04, id1]` may still be aggregated for windows `{:55, :05}` and `{:00, :10}`. This is because a watermark *does not guarantee that data arriving after the threshold is dropped.* It only *guarantees that data arrviving before the threshold is never dropped.* See \"Semantic guarantees with watermarks\" in the book.\n",
    "\n",
    "\n",
    "* There are errors in Figure 8-10, that is why our final DataFrame looks different from that in the book. The event `[:15, id1]` will not be counted in the window `{:05, :15}`, because [the window ends are exclusive](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html#pyspark.sql.functions.window). So, the count for this sensor ID and window will be 1 and not 2 as in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554244b6-57a4-4fd7-969c-fa16f616bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query_windowing.stop()\n",
    "streaming_query_windowing.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5834d0-21b1-4f11-8ae7-19b6ae3112d8",
   "metadata": {},
   "source": [
    "To restart the query, first, clear the `events` Kafka topic using [AKHQ](http://localhost:8086/ui/docker-kafka-server/topic/events/data?sort=Oldest&partition=All), then, restart the query by running the cell in which `streaming_query_windowing` is defined and the subsequent cell in which the events are sent to Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076a5b0-ce99-43f7-93cd-0746a12a1b2c",
   "metadata": {},
   "source": [
    "## Streaming Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775c186-d16b-401a-8cfe-0854b5a21c52",
   "metadata": {},
   "source": [
    "### Stream-Static Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7947b1d7-c1e7-418b-aa44-7c9c9c3ad507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the stream-static join example close to that one from the book\n",
    "\n",
    "# A static DataFrame of impressions (i.e., all the ads to be shown) \n",
    "df_impressions = spark.createDataFrame(data=[(i, chr(65+i)) for i in range(26)],\n",
    "                                       schema=\"`adId` long, `impressionDescription` string\")\n",
    "df_impressions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac5807-c3e5-495e-af59-c5c01f3d83cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A streaming DataFrame from the rate data source (timestamp-counter pairs)\n",
    "# that we interpret as a stream of clicks.  The counter value is interpreted\n",
    "# as the adId that was clicked.\n",
    "#\n",
    "# Rate source docs: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources\n",
    "df_clicks = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "df_impressions_clicks_join = df_clicks.join(df_impressions, df_clicks.value == df_impressions.adId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8b039-21a2-4fd0-ac76-1fd64f93589d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir_impressions_clicks_join = f\"/tmp/spark-streaming-checkpoints-impressions-clicks-join-{uuid1()}\"\n",
    "\n",
    "straming_query_impressions_clicks_join = (df_impressions_clicks_join\n",
    "                                          .writeStream\n",
    "                                          .format(\"console\")\n",
    "                                          .option(\"truncate\", False)\n",
    "                                          .option(\"checkpointLocation\", checkpoint_dir_impressions_clicks_join)\n",
    "                                          .outputMode(\"append\")\n",
    "                                          .trigger(processingTime=\"2 seconds\")\n",
    "                                          .start())\n",
    "straming_query_impressions_clicks_join.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c60b80-1512-4d46-8444-b8a0ac95eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "straming_query_impressions_clicks_join.stop()\n",
    "straming_query_impressions_clicks_join.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c950c-bdac-4277-9199-68ff87368d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
