{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f613059-8962-4709-88bc-e6899cd987c2",
   "metadata": {},
   "source": [
    "# Chapter 5. Spark SQL and DataFrames: Interacting with External Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe9665-e6f4-457a-a53c-80bf77cef506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"3g\").config(\"spark.jars\", \"/home/kolesnik/work/learning/spark/jars/postgresql.jar\").config(\"spark.sql.catalogImplementation\",\"hive\").appName(\"SparkSQLExampleApp\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7056bf7a-074a-4cbd-8f24-7226704538be",
   "metadata": {},
   "source": [
    "## Spark SQL and Apache Hive, User-Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6638c96-3a2e-48a1-8d07-11c679779625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Create and register the `cubed()` user-defined function\n",
    "def cubed(s):\n",
    "    return s * s * s\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "\n",
    "# Generate a temporary view and query it using UDF\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n",
    "spark.sql(\"select id, cubed(id) as id_cubed from udf_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce060e-b110-43ff-9a5a-993ebdcef3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas UDF\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the cubed function\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bbd444-9b1e-4640-995a-5e9c4a63f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for a pandas_udf executed with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69839b77-f301-48ab-9216-b9e49e782e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame and Execute function as a Spark vectorized UDF\n",
    "df = spark.range(1, 4)\n",
    "df_cubed = df.select(\"id\", cubed_udf(df.id).alias(\"cubed_id\"))\n",
    "df_cubed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab6836-d60c-4abf-8979-b63fbc0d2fd5",
   "metadata": {},
   "source": [
    "## Querying with the Spark SQL Shell, Beeline, and Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f68ce7-8a1f-486f-9a3d-ec4708b69198",
   "metadata": {},
   "source": [
    "### Using the Spark SQL Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ecdc5b-a4b2-4b59-a67d-74e44216753d",
   "metadata": {},
   "source": [
    "```\n",
    "CREATE TABLE people (name STRING, age int);\n",
    "insert INTO people VALUES (\"Michael\", NULL);\n",
    "insert INTO people VALUES (\"Andy\", 30);\n",
    "insert INTO people VALUES (\"Samantha\", 19);\n",
    "show tables;\n",
    "select * from people;\n",
    "select * from people where age < 20;\n",
    "select * from people where age is null;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c51d5-b676-4fca-a2b5-e5f58fe130d6",
   "metadata": {},
   "source": [
    "### Running the Thrift server using only pyspark package\n",
    "```\n",
    "park-class org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 spark-internal\n",
    "\n",
    "beeline\n",
    "\n",
    "!connect jdbc:hive2://localhost:10000\n",
    "```\n",
    "Use `<user>@<host>` for username and blank for password."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3430e1-74a2-4aa2-857e-1076fc15f0b5",
   "metadata": {},
   "source": [
    "## External Data Sources, PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e781b7-8a52-44c8-aa8d-f385b01d0fef",
   "metadata": {},
   "source": [
    "Note that a JAR with the corresponding PostgreSQL JDBC driver was added to the classpath while buiding a `SparkSession` ojbect (see `config(\"spark.jars\", ...)` call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00c16b-b1a5-42b2-af04-869ad33a01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_cubed\n",
    " .write\n",
    " .format(\"jdbc\")\n",
    " .option(\"driver\", \"org.postgresql.Driver\")  # This line is missing in the book and leads to the `java.sql.SQLException: No suitable driver` exception\n",
    " .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\")\n",
    " .option(\"user\", \"postgres\")\n",
    " .option(\"password\", \"example\")\n",
    " .option(\"dbtable\", \"public.cubed\")\n",
    " .mode(\"overwrite\")\n",
    " .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01637413-aa47-43c6-b2f0-7123c85dc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"driver\", \"org.postgresql.Driver\")  # This line is missing in the book and leads to the `java.sql.SQLException: No suitable driver` exception\n",
    " .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\")\n",
    " .option(\"user\", \"postgres\")\n",
    " .option(\"password\", \"example\")\n",
    " .option(\"dbtable\", \"public.cubed\")\n",
    " .load()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9ed52-9d67-4c87-b29b-45fd01e43a1a",
   "metadata": {},
   "source": [
    "## Higher-Order Functions in DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c76b3-e3e9-4740-b92e-86a341b43fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data. Series of temperature values in Celsius\n",
    "schema = \"`celsius` array<int>\"\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "# Show the DataFrame\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4616ae5-16c1-457e-9ea8-66a3a081bb85",
   "metadata": {},
   "source": [
    "### transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12df2cf-d566-4827-9885-00686c31dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Fahrenheit from Celsius for an array of temperatures\n",
    "spark.sql(\"select celsius, transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit from tC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d74e2-2bac-459e-954f-92f5e8a5d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c.select(\"celsius\", F.transform(\"celsius\", lambda t: ((t * 9) / 5) + 32).alias(\"fahrenheit\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e4652-50f9-41fe-8952-582083a2fe2d",
   "metadata": {},
   "source": [
    "### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226c7bc-5169-443b-a3b7-890071f31a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"select celsius, filter(celsius, t -> t > 38) as high from tC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084bcbb-bdeb-4a2d-8414-645a75e02188",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c.select(\"celsius\", F.filter(\"celsius\", lambda t: t > 38).alias(\"high\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80618862-3173-40d4-91b5-d43456122fbc",
   "metadata": {},
   "source": [
    "### exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a14aae-6ebf-431d-a93e-1d7be152f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a temperature of 38C in the array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "exists(celsius, t -> t = 38) as threshold\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a18fac-2822-427f-83fb-b9e95eefc4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c.select(\"celsius\", F.exists(\"celsius\", lambda x: x == 38).alias(\"threshold\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0c8d7-59ae-4b1d-a5ac-a574fb21d9ef",
   "metadata": {},
   "source": [
    "### reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7da0f-1a07-4249-a320-3bc7b5ca0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "reduce(\n",
    "celsius,\n",
    "0,\n",
    "(t, acc) -> t + acc,\n",
    "acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    ") as avgFahrenheit\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad198e3-791f-49b8-bb86-ea950a81e8ae",
   "metadata": {},
   "source": [
    "There is no a corresponding `reduce()` function in `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25004b56-13be-41dd-94ae-ecb542ca6027",
   "metadata": {},
   "source": [
    "## Common DataFrames and Spark SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e661277-2efb-4959-84d5-73bae835e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdelaysFilePath = \"../data/departuredelays.csv\"\n",
    "airportsnaFilePath = \"../data/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c22bb5-b4a2-4a07-95a2-c46b0c71b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain airports data set\n",
    "airportsna = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    " .load(airportsnaFilePath))\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8f3c7-0bc5-41eb-9a27-0ad89dd507f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain departure delays data set\n",
    "departureDelays = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\")\n",
    " .load(tripdelaysFilePath))\n",
    "departureDelays = (departureDelays\n",
    " .withColumn(\"delay\", F.expr(\"CAST(delay as INT) as delay\"))\n",
    " .withColumn(\"distance\", F.expr(\"CAST(distance as INT) as distance\")))\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bc966-54c5-4eb6-a467-8b79aac2cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary small table\n",
    "foo = (departureDelays\n",
    " .filter(F.expr(\"origin == 'SEA' and destination == 'SFO' and date like '01010%' and delay > 0\")))\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02bb9c-a4df-46e2-9b53-16d0674eed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()\n",
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()\n",
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e71b67-ad7f-41b1-8d25-5fc0bfd389ec",
   "metadata": {},
   "source": [
    "### Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0e508-257b-47ad-9741-8400a2dc262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union of foo and departureDelays creates duplicate entries, because foo rows were taked from departureDelays\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b589a-5f78-4d1a-aca9-5c4f8994af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the union (filtering for SEA and SFO in a specific time range)\n",
    "bar.filter(F.expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b99d8-96f6-4c3e-9ed5-adc22582cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the union in SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM bar\n",
    "WHERE origin = 'SEA'\n",
    "AND destination = 'SFO'\n",
    "AND date LIKE '01010%'\n",
    "AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d932200-0651-441b-a35e-c3b30fd938fa",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc01b4e-ca9c-40f4-94a9-2ba624947e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsna.show(2)\n",
    "foo.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b1490-761e-4611-a9ec-d97b904b6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join departure delays data (foo) with airport info (default inner join)\n",
    "foo.alias(\"f\").join(airportsna.alias(\"a\"), airportsna.IATA == foo.origin).select(\"a.City\", \"a.State\", \"f.date\", \"f.delay\", \"f.distance\", \"f.destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8526a99-bb7c-4131-b744-4633cfbded0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same in SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination\n",
    "FROM foo f\n",
    "JOIN airports_na a\n",
    "ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959028e3-880d-4e10-a3b2-cd68a8214ec7",
   "metadata": {},
   "source": [
    "### Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd8855-dab7-448d-a16b-0e12efea5c5f",
   "metadata": {},
   "source": [
    "Note that for \"create table ... as select\" to work we set catalog implementation to \"hive\" while buiding a `SparkSession` ojbect (see `config(\"spark.sql.catalogImplementation\",\"hive\")` call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570363d-9760-405e-aae1-da06a94656b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS departureDelaysWindow\")\n",
    "spark.sql(\"\"\"\n",
    "create table departureDelaysWindow as\n",
    "select origin, destination, sum(delay) as TotalDelays\n",
    "  from departureDelays\n",
    "  where origin in ('SEA', 'SFO', 'JFK')\n",
    "    and destination in ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "  group by origin, destination\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f997dd-dad0-4a94-a361-4fb3c31f6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the origin airports find the three destinations that experienced the most delays\n",
    "spark.sql(\"\"\"\n",
    "SELECT origin, destination, TotalDelays, rank\n",
    "  FROM (\n",
    "    SELECT origin, destination, TotalDelays, dense_rank()\n",
    "        OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank\n",
    "      FROM departureDelaysWindow\n",
    "  )\n",
    "WHERE rank <= 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936255e6-c9e8-47bd-a3d1-af39785c2f7c",
   "metadata": {},
   "source": [
    "More on Spark's window functions [here](https://www.databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f34006-5a47-4983-a8a5-eb847b8947e4",
   "metadata": {},
   "source": [
    "### Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55005a40-64cd-4d4a-a960-a3d326da8dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb75f6b-3695-4cc6-8301-17dbec3a8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns\n",
    "foo2 = foo.withColumn(\"status\", F.expr(\"case when delay <= 10 then 'On-time' else 'Delayed' end\"))\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272bc2b-415e-4b6b-86be-1248426124e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same transformation using DataFrame API\n",
    "foo22 = foo.withColumn(\"status\", F.when(foo.delay <= 10, \"On-time\").otherwise(\"Delayed\"))\n",
    "foo22.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c70965-91a8-4e89-898a-7a2c7463aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0251149-5476-4d31-8c33-82bccc2b8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2da7c-1b09-4839-8661-18e751894dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting\n",
    "spark.sql(\"\"\"\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "  FROM departureDelays\n",
    "WHERE origin = 'SEA'\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68060dd8-5fd1-4322-a84c-7002828345e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM (\n",
    "  SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "    FROM departureDelays WHERE origin = 'SEA'\n",
    ")\n",
    "PIVOT (\n",
    "  CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    "  FOR month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b762aa-9e6b-41fb-bf06-037f0d1a336d",
   "metadata": {},
   "source": [
    "More on pivoting [here](https://www.databricks.com/blog/2018/11/01/sql-pivot-converting-rows-to-columns.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643ed5e-bcc7-41b2-902d-fd2e7fce2fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
