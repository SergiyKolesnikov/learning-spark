{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5878bb77-0643-4a3a-bca1-0c1874556758",
   "metadata": {},
   "source": [
    "# Chapter 10. Machine Learning with MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdd244-5249-4482-af76-bb6dfd98a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  .master(\"local[4]\")\n",
    "  .appName(\"MachineLearning\")\n",
    "  .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64d999-61d9-44e1-b4ee-bda255890d35",
   "metadata": {},
   "source": [
    "## Designing Machine Learning Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0e7b4-c6c1-44f0-b3e5-cd5ec51adb78",
   "metadata": {},
   "source": [
    "### Data Ingestion and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2573e7-347a-4d6b-9fbf-f1cca1c014e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"../data/sf-airbnb/sf-airbnb-clean.parquet/\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\", \n",
    "                \"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a9ba1-7ac7-41ce-b057-11b9ba9a8c55",
   "metadata": {},
   "source": [
    "### Creating Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa4d25-30e8-4bba-b829-58923f38aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456e780-4937-42b8-899a-a6d603bbcabf",
   "metadata": {},
   "source": [
    "### Preparing Features with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbce06-bad4-4b17-942d-2a8283e7512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862262e4-cbdd-4e25-aa6f-422cfb384ebe",
   "metadata": {},
   "source": [
    "### Using Estimators to Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77212ac-dd46-43e0-96b8-a9917b8d48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95421101-382d-4cbf-90f8-68270cd8ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the learned parameters\n",
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "print(f\"\"\"The formula for the linear regression line is \n",
    "price = {m}*bedrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563b337-32ef-43ee-ace6-7be84ca29436",
   "metadata": {},
   "source": [
    "### Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493dad4-5caf-48c7-a1a2-97b894234e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipelin\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e6cbc-8173-40c6-95ff-f745b785515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the test data set\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ab6bc-f1e4-4e40-8080-c1d9c3b96f4e",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ea2b6-f5aa-415c-a325-a8111b49cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, \n",
    "                           outputCols=oheOutputCols)\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, \n",
    "                               outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf6f6d6-a872-4a5e-94df-c17eca47d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same as in the previous cell, but using RFormula\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "rFormula = RFormula(formula=\"price ~ .\", \n",
    "                    featuresCol=\"features\", \n",
    "                    labelCol=\"price\", \n",
    "                    handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94a4bb-a5f0-40c8-a1fb-368f6dac40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "# Or use RFormula\n",
    "# pipeline = Pipeline(stages = [rFormula, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae62f7c-fc35-4011-92f1-2783c12b1c27",
   "metadata": {},
   "source": [
    "### Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6aa34-e8de-4303-8739-974b46dc8746",
   "metadata": {},
   "source": [
    "#### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c82cb3-797e-430a-a633-2bd08ea57145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "  predictionCol=\"prediction\", \n",
    "  labelCol=\"price\", \n",
    "  metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c804a-9c42-421c-95f2-e47497f6fd90",
   "metadata": {},
   "source": [
    "##### R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d0af4-4560-476b-be7b-1e44e626b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb50044-af28-490b-9e7c-2d4f148a334e",
   "metadata": {},
   "source": [
    "### Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b8276-f7bd-42a9-b1b9-199fe67bee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = \"../data_output/lr-pipeline-model\"\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f693d5c-c581-4262-bb59-c56e024127b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)\n",
    "savedPipelineModel.transform(testDF).select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d4c4c-9470-4d60-a876-0a0fe9f49767",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a46c7-8bce-4e94-8bdc-1ec469645d38",
   "metadata": {},
   "source": [
    "### Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a18bdcf-2ebe-4d82-8880-e5ee7c86417d",
   "metadata": {},
   "source": [
    "#### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73451ee1-40a3-4535-871f-b802a52dd4f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol=\"price\")\n",
    "\n",
    "# Filter for just numeric columns (and exclude price, our label)\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Combine stages into pipeline\n",
    "stages = [stringIndexer, vecAssembler, dt]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(trainDF) # This line should error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1776e-82e1-4114-b615-173a3d68b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase `maxBins` to handle the discretization of the categorical columns\n",
    "dt.setMaxBins(40)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d92d0-3ab4-4292-bc8b-9942a950d948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the if-then-else rules learned by the decision tree\n",
    "dtModel = pipelineModel.stages[-1]\n",
    "print(dtModel.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e04f2ff-652e-463a-890e-824b244fb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature importance scores from our model to see the most important features\n",
    "import pandas as pd\n",
    "\n",
    "featureImp = pd.DataFrame(\n",
    "  list(zip(vecAssembler.getInputCols(), dtModel.featureImportances)),\n",
    "  columns=[\"feature\", \"importance\"])\n",
    "featureImp.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82453e51-62c1-402b-935e-0e93253975e2",
   "metadata": {},
   "source": [
    "#### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb5d0e-34aa-4665-9847-43cf9d62daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c41a5-5289-4d23-9bc4-9ac9b08a69d8",
   "metadata": {},
   "source": [
    "### k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170268e-7fca-492e-a261-5bb4193e860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pipeline estimator\n",
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fe645-31ce-450e-ba35-951a85da6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our hyperparameter grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "            .addGrid(rf.numTrees, [10, 100])\n",
    "            .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417cc42-a2f0-4202-9be1-0c8c7604e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to evaluate each of the models to determine which one performed best\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", \n",
    "                                predictionCol=\"prediction\", \n",
    "                                metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580157f-9804-4763-96fb-1ae471be488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-fold cross-validation\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    evaluator=evaluator, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    numFolds=3, \n",
    "                    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfa093-2fd9-473f-a99b-09249577fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308a551-1e29-4101-90d6-ff79a29f905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the results of the cross-validator\n",
    "list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f098d89-2e39-4f48-b043-f8566e3b8c33",
   "metadata": {},
   "source": [
    "### Optimizing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306809fd-5a4c-4d1e-a99f-00c122b1607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "cvModel = cv.setParallelism(10).fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fb5ed-c66c-419e-88fd-24b26972865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=rf, \n",
    "                    evaluator=evaluator, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    numFolds=3, \n",
    "                    parallelism=10, \n",
    "                    seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac769151-19a3-4f75-a9ac-8895aa262e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf99f9-4376-4e1a-8ce3-b13589b755a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
